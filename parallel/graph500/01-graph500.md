# Building a Raspberry Pi Cluster

#### Hardware

There are quite a few tutorials out there on how to set up a distributed cluster of sorts. When first getting started, it can be confusing to know which one to pick, what to actually do (or not do), and how to do it. The biggest challenge is actually bookkeeping. At the absolute minimum, you will need:

1. At least two computers, they can be almost any make but it is easiest to have two of the same (Raspberry Pis)
2. One router
3. One switch
4. Enough Ethernet cables to connect the computers to the switch, the switch to the router, and probably the router to the internet

The setup would look something like

<img src="http://youngindiainformation.com/wp-content/uploads/2011/06/modem-router-swtich-net-2.jpg" align="lefgt" width="400" height="400">

That is all the hardware you need to get a cluster up and running!

#### Software

We are currently using a Linux distribution called [Raspbian](https://www.raspbian.org) which is a derivative of the Debian Linux distribution:

<img src="http://futurist.se/gldt/wp-content/uploads/12.10/gldt1210.svg" align="lefgt" width="600" height="1200">

Some other popular distributions include Ubuntu, CentOS, Mint, and SUSE. Slackware was my first love.

On our Debian-based systems, we have been using apt-get to grab pre-compiled software packages from the "cloud" -- for free I might add. For us to run Graph 500 across our of our pis, we will be using a library called MPI (Message Passing Interface). MPI has become the defacto standard in high performance computing for distributed memory clusters. One implementation that we can get from apt-get is called [Open MPI](https://www.open-mpi.org). MPI has rules for the C and Fortran languages.

To install the software we need to make our Pi cluster, run the mpi_install script in parallel/graph500/setup/mpi_install. I will provide you with an IP address to give to the script. The first time, while you are still at your stations you will verify that your repository is located in your home directory. 

```
ls ~/summer-2015
```

If you get a message

```
ls: cannot access /home/pi/summer-2015: No such file or directory
```

we will need to move your repository first!

Once every has their repos located at ~/summer-2015, go ahead and navigate to the setup directory and run the mpi_install script like

```
cd ~/summer-2015/parallel/graph500/setup
install=true ./mpi_install 123.456.7.890
```

The install will take 5 to 10 minutes. Some of the software you all have already installed but this is everything that is needed

```
sudo apt-get update -y
sudo apt-get upgrade -y
sudo apt-get install build-essential -y
sudo apt-get install gfortran -y
sudo apt-get install openmpi-dev -y
sudo apt-get install openmpi-bin -y
```

There are a few other cool tools installed too

```
sudo apt-get install vim -y
sudo apt-get install nmon -y
sudo apt-get install apt-transport-https ca-certificates -y
sudo wget http://goo.gl/rsel0F -O /etc/apt/sources.list.d/rpimonitor.list
sudo apt-key adv --recv-keys --keyserver keyserver.ubuntu.com 2C0D3C0F 
sudo apt-get update -y
sudo apt-get install rpimonitor -y
sudo apt-get update -y
sudo apt-get upgrade -y
sudo /usr/share/rpimonitor/scripts/updatePackagesStatus.pl
```

Namely nmon and rpimonitor (oh and vim -- which is way better than emacs).

#### nmon

nmon is a fun little tool that is a real time terminal-based resource monitor. You can open up a terminal window, type "nmon" and it will come up asking what you wish to display. Typically, a choice of "c" for CPU load is what we are curious about.

<img src="http://nmon.sourceforge.net/docs/lmon12e_colour_400.jpg" align="lefgt" width="600" height="400">

#### RPi Monitor

RPi Monitor is a cool suite of tools to help asses the health and status of your raspberry pi. Once installed, you should be able to navigate to "localhost:8888" in your browser to start the monitor and to see what is going on.

#### MPI Hello World

Once our installation has completed, we are going to test out our MPI installation and make sure everything is happy and running. Try

```
cd ~/summer-2015/parallel/graph500/hello
mpif90 mpi_hello.F90 -o hello
mpirun -np 4 --hostfile myhostfile ./hello
```

First, we are compiling the Fortran code mpi_hello.F90 using the MPI gfortran wrapper mpif90 and producing the executable called "hello".

Next, once we have successfully compiled our hello executable, we are using the mpirun process launcher to spawn 4 mpi processes all on your local machine to run the hello executable at the same time. We are providing a hostfile for mpirun to know where to run the mpi processes.

#### Local Graph 500

Now, you can also test out the Graph 500 executable that we are going to be running as well

```
cd ~/summer-2015/parallel/graph500/graph500
mpirun -np 4 ./graph500_mpi_one_sided 1
```

You should see 64 iterations and then a summary once complete. Try running with 1, 2, or 4 MPI processes by changing the number after "-np". Try scaling the problem up from 1 to 2, 3, ..., up to 10 or so by changing the second number on the command line.

## Network Logistics

At this point, we have the right software installed and we have tested that it is working as expected. The next step is for us to hook up all 32 pis to our 48 port switch, acquire an IP address and report it to our "reference" pi.

We will break up into three groups of 8 -- each group is a set of big tables. When your group is called, you will shutdown your pi (save any of your work first!) from the command line with

```
sudo shutdown -h now
```

Wait until the green light stops flashing and only the red light remains. 

***Take out your USB wifi adapter*** and put it in your ziplock bag for safe keeping!

We will rearrange the room so that there will be four stations for you all to plug in an Ethernet cable from the switch into your pi, restart your pi using a monitor from one of the stations. Wait until it boots up, open a terminal window and type

```
hostname -I
```
Repeatedly until you get an IP address that starts with 192 -- not 169. At that point, you will need to run a command to send your hostname and IP address to our reference pi:

```
cd ~/summer-2015/parallel/graph500/setup
./mpi_install <reference pi IP address which I will give you>
```

This will do several things:

1. It will set up your ssh keys so that we can switch between the pis without needing a password.
2. It will copy a file that is comprised of your hostname and IP address to the reference pi.
3. It will set up a ~/.bashrc  and ~/.vimrc file for you -- neither are necessary for our graph500 but it makes it easier for me to navigate your pi if we run into trouble.

Try running

```
source ~/.bashrc
```

and watch what happens!

Once everyone has hooked up their pis and sent their hostname and IP address to the reference pi, I will be running a few scripts to set up files that include

1. creating a master /etc/hosts file
2. creating hostfile for mpirun
3. distributing the authorized keys for ssh to allow for passwordless access
4. distributing the /etc/hosts file

From there, we will try to run the MPI hello executable across all the pis at the same time! If that works, we are all set to run the graph 500 benchmark!



# Running the Graph 500 Benchmark

<img src="https://www.alcf.anl.gov/files/field/image/graph500-logo.jpg" align="lefgt" width="600" height="400">

#### What is Graph 500?


[Graph 500](http://www.graph500.org) is a relatively new benchmark meant to represent data intensive workloads in the fields of Cybersecurity, Medical Informatics, Data Enrichment, Social Networks, and Symbolic Networks. 

<img src="http://corte.si/%2Fposts/privacy/neighbourhoods-of-trust/images/full.png" align="lefgt" width="600" height="600">

Graph algorithms are a core part of many analytics workloads. The argument by over 50 international HPC experts is that a new set of benchmarks is needed in order to guide the design of hardware architectures and software systems intended to support such applications and to help procurements. The hope is that Graph 500 can rise to that challenge and become a compliment to the well known [Top 500 List](http://www.top500.org) which runs a benchmark called HPL (High Performance LINPACK).

On the Graph 500 website, a benchmark specification is provided. Problem classes ranging from toy (level 10) on up to huge (level 15) are outlined. We are hoping that when all 24 of the Raspberry Pis (along with a few extra) are connected together, we ***might*** be able to run the toy class of the problem.

The benchmark is measured in GTEPS (Giga - Transversed Edges Per Second) and by the number of vertices (given by 2^(SCALE)).

With a total of 32 Pis with 4 cores each, we can run efficiently a total of 128 MPI processes. The benchmark will be started towards the end of the day and it will be run on increasing problem scales until we either run out of time or we run out of memory!


## Challenge

1. Try for a spot on the November 2015 list -- and, if we are lucky, above at least one other entry!
2. Over the next six days, work up to the toy class which consists of 67,108,864 vertices taking up over 17 GB RAM across all 32 Raspberry Pis.


# MPI Thought Experiments While You Wait

You all will be broken into teams of 8. Your job is to come up with solutions to the following two thought experiments. These represent some important and fundamental concepts in distributed parallel computing which are leveraged heavily in MPI libraries.

### Distributed Math Operations

1. Your team is given a zip lock bag of 80 numbers. 
2. Each team member can be thought of as an MPI process. 
3. Distribute 10 numbers to each team member. 
4. Your goal is to somehow sum all 80 numbers such that one team member has the final result.
5. Each team member, may (or may not), participate in the actual summation.
5. Design at least two different algorithms to come up with the sum of all 80 numbers. 
6. Note that communication of any numbers from one team member to another must be taken into account.
7. Design your alogrithms such that they are more efficient than what the other teams come up with.
8. Can you come up with a third algorithm that is the least efficient?

### The Spaghetti Philosophers

<img src="https://d1u2s20mo6at4b.cloudfront.net/wp-content/uploads/dining-philosophers.jpg" align="lefgt" width="400" height="400">

Five silent philosophers sit at a round table with bowls of spaghetti. Forks are placed between each pair of adjacent philosophers.

Each philosopher must alternately think and eat. However, a philosopher can only eat spaghetti when they have both left and right forks. Each fork can be held by only one philosopher and so a philosopher can use the fork only if it is not being used by another philosopher. After a philosopher finishes eating, they need to put down both forks so the forks become available to others. A philosopher can take the fork on their right or the one on their left as they become available, but cannot start eating before getting both of them.

Eating is not limited by the remaining amounts of spaghetti or stomach space; an infinite supply is assumed.

The problem is how to design a discipline of behavior (a concurrent algorithm) such that each philosopher will not starve; i.e., can forever continue to alternate between eating and thinking, assuming that any philosopher cannot know when others may want to eat or think.

1. Can you come up with an algorithm that will continue forever if the philosophers do not communicate to each other?
2. Can you come up with an algorithm that will continue forever if the philosophers can communicate to each other?

You may want to assign numbers to the philosophers and to the forks as well for easier reference.

# Overall Results

![Alt text](https://rawgit.com/CODE-at-TACC/summer-2015/master/parallel/graph500/graph500/results/04_run/results.svg)
<img src="https://rawgit.com/CODE-at-TACC/summer-2015/master/parallel/graph500/graph500/results/04_run/results.svg">

We were able to run all 32 pis together at a scale of 26 for a final score of 35.5917 GTEPS (Giga - Traversed Edges Per Second)! If officially accepted, this could place us at position 62 on the Graph 500 Benchmark.
